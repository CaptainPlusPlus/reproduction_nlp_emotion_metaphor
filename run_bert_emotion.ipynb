{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYbfhl74JRH1"
      },
      "source": [
        "### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_X3UROMJRH4"
      },
      "outputs": [],
      "source": [
        "!pip3 install transformers\n",
        "!pip3 install datasets\n",
        "!pip3 install accelerate -U\n",
        "!pip3 install transformers[torch]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZG7esYnJRH7"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VQvfaSsJRH7"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer\n",
        "from transformers import AutoTokenizer, Trainer, TrainingArguments, AutoModelForSequenceClassification, BertTokenizerFast, BertForSequenceClassification\n",
        "from transformers.trainer_callback import EarlyStoppingCallback\n",
        "import torch\n",
        "import accelerate\n",
        "import numpy as np\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.metrics import f1_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6o10cDxJRH8"
      },
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N18vu3zxJRH8"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset('csv', data_files='data/EMOBANK/emobank.csv')\n",
        "\n",
        "train_dataset = dataset['train'].filter(lambda example: example['split'] == 'train')\n",
        "test_dataset = dataset['train'].filter(lambda example: example['split'] == 'test')\n",
        "dev_dataset = dataset['train'].filter(lambda example: example['split'] == 'dev')\n",
        "\n",
        "\n",
        "def normalize_values(dataset):\n",
        "    for key in ['V', 'A', 'D']:\n",
        "        dataset[key] = (dataset[key] - 1) / 4\n",
        "    return dataset\n",
        "\n",
        "train_dataset = train_dataset.map(normalize_values)\n",
        "test_dataset = test_dataset.map(normalize_values)\n",
        "dev_dataset = dev_dataset.map(normalize_values)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpmU5PGYJRH9"
      },
      "source": [
        "### Preprocess Data and Tokenize input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUbf8ur1JRH9"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "dev_dataset = dev_dataset.filter(lambda example: example['text'] is not None)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "# For Python 3.9\n",
        "    # tokenized = tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "    # return {key: value.numpy() for key, value in tokenized.items()}\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "dev_dataset = dev_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Clean data\n",
        "dev_dataset = dev_dataset.filter(lambda example: example['text'] is not None)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjJ-n2ieJRH-"
      },
      "source": [
        "### Format Dataset for 3 lavel training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L00E3_HDJRH-"
      },
      "outputs": [],
      "source": [
        "def format_dataset(example):\n",
        "    example['labels'] = [example['V'], example['A'], example['D']]\n",
        "    return example\n",
        "\n",
        "train_dataset = train_dataset.map(format_dataset)\n",
        "test_dataset = test_dataset.map(format_dataset)\n",
        "dev_dataset = dev_dataset.map(format_dataset)\n",
        "\n",
        "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "dev_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg4yddezJRH_"
      },
      "source": [
        "### Initialize Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubdnQhRtJRH_"
      },
      "outputs": [],
      "source": [
        "from transformers import BertConfig, BertForSequenceClassification\n",
        "\n",
        "config = BertConfig.from_pretrained(\"bert-base-cased\", num_labels=3)  # 3 for V, A, D\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-cased\", config=config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xjF6wv3JRH_"
      },
      "source": [
        "### Setup training and metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZdZkLchJRH_"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=12,\n",
        "    learning_rate=5e-5,\n",
        "    output_dir='./results',\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=500,\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    evaluation_strategy=\"epoch\", \n",
        "    save_strategy=\"epoch\", \n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=dev_dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OImNLWFrJRIA"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMTzG__RJRIA"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXQWE4ybJRIA"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vG_xNnQ9JRIA",
        "outputId": "3047f3a3-9893-4367-cc55-89bb1c9a459f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pearson r values: Valence: 0.7534930780797127, Arousal: 0.5561622928181004, Dominance: 0.4736017701452015\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "predictions = trainer.predict(test_dataset)\n",
        "predicted_values = np.array(predictions.predictions)\n",
        "\n",
        "ground_truth = test_dataset['labels']\n",
        "\n",
        "pearson_v = pearsonr(predicted_values[:, 0], ground_truth[:, 0])[0]\n",
        "pearson_a = pearsonr(predicted_values[:, 1], ground_truth[:, 1])[0]\n",
        "pearson_d = pearsonr(predicted_values[:, 2], ground_truth[:, 2])[0]\n",
        "\n",
        "print(f\"Pearson r values: Valence: {pearson_v}, Arousal: {pearson_a}, Dominance: {pearson_d}\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Readjust and run model for Metaphor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XH4dZn0xdYAM"
      },
      "outputs": [],
      "source": [
        "train_path = 'data/VUA/VUA_formatted_train.csv'\n",
        "eval_path = 'data/VUA/VUA_formatted_val.csv'\n",
        "test_path = 'data/VUA/VUA_formatted_test.csv'\n",
        "\n",
        "def load_dataset(train, eval, test, encoding):\n",
        "    train_df = pd.read_csv(train, encoding=encoding)\n",
        "    eval_df = pd.read_csv(eval, encoding=encoding)\n",
        "    test_df = pd.read_csv(test, encoding=encoding)\n",
        "\n",
        "    return train_df, eval_df, test_df\n",
        "\n",
        "train_df, eval_df, test_df = load_dataset(train_path, eval_path, test_path, encoding='ISO-8859-1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFbpPT-AdVe1"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 128\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "def preprocess_data(df):\n",
        "    tokenized_sentences = []\n",
        "    attention_masks = []\n",
        "    verb_labels = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        sentence = row['sentence']\n",
        "        label = row['label']\n",
        "\n",
        "        # Tokenize the sentence and get the respective wordpiece token positions\n",
        "        tokens = tokenizer.tokenize(sentence)[:MAX_LEN - 2]  # Account for BERT model [CLS] and [SEP]\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "        mask = [1] * len(input_ids)\n",
        "\n",
        "        # Padding\n",
        "        while len(input_ids) < MAX_LEN:\n",
        "            input_ids.append(0)\n",
        "            mask.append(0)\n",
        "\n",
        "        tokenized_sentences.append(input_ids)\n",
        "        attention_masks.append(mask)\n",
        "\n",
        "        # Keep the labels as integers\n",
        "        verb_labels.append(label)\n",
        "\n",
        "    return tokenized_sentences, attention_masks, verb_labels\n",
        "\n",
        "train_encodings, train_masks, train_labels = preprocess_data(train_df)\n",
        "eval_encodings, eval_masks, eval_labels = preprocess_data(eval_df)\n",
        "test_encodings, test_masks, test_labels = preprocess_data(test_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Rt8LUWEdOJy"
      },
      "outputs": [],
      "source": [
        "def create_hf_dataset(encodings, masks, labels):\n",
        "    return Dataset.from_dict({\n",
        "        'input_ids': encodings,\n",
        "        'attention_mask': masks,\n",
        "        'labels': labels\n",
        "    })\n",
        "\n",
        "train_dataset = create_hf_dataset(train_encodings, train_masks, train_labels)\n",
        "eval_dataset = create_hf_dataset(eval_encodings, eval_masks, eval_labels)\n",
        "test_dataset = create_hf_dataset(test_encodings, test_masks, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0xp0nyodHVY"
      },
      "outputs": [],
      "source": [
        "model = BertForSequenceClassification.from_pretrained('./results/checkpoint-6048', problem_type=\"single_label_classification\")\n",
        "model.classifier = torch.nn.Linear(in_features=768, out_features=2)\n",
        "model.num_labels = 2\n",
        "\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    f1 = f1_score(labels, predictions)\n",
        "    r, _ = pearsonr(labels, predictions)\n",
        "    return {\"f1\": f1, \"pearson_r\": r}\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=1,\n",
        "    logging_dir='./logs_metaphor',\n",
        "    logging_steps=2000,\n",
        "    save_steps=2000,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_total_limit=2,\n",
        "    learning_rate=2e-5,\n",
        "    remove_unused_columns=False,\n",
        "    output_dir=\"./drive/MyDrive/results/base_metaphor\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "Vddkztqcq4eU",
        "outputId": "25b97b25-1b1b-4f2b-b2de-64098b97655c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1940' max='1940' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1940/1940 06:41, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1940, training_loss=0.5510524474468428, metrics={'train_runtime': 402.2288, 'train_samples_per_second': 38.575, 'train_steps_per_second': 4.823, 'total_flos': 1020607783741440.0, 'train_loss': 0.5510524474468428, 'epoch': 1.0})"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "collapsed": true,
        "id": "fx4KESY4dEcX",
        "outputId": "40680c9a-f645-47d7-8599-9c4cf687161d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='951' max='216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [216/216 01:01]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.544163703918457, 'eval_f1': 0.35887096774193544, 'eval_pearson_r': 0.2600936847597024, 'eval_runtime': 47.4869, 'eval_samples_per_second': 123.676, 'eval_steps_per_second': 15.478, 'epoch': 1.0}\n"
          ]
        }
      ],
      "source": [
        "results = trainer.evaluate()\n",
        "\n",
        "results_on_test_data = trainer.evaluate(test_dataset)\n",
        "print(results_on_test_data)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
